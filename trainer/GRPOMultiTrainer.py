from typing import Callable, Optional, Union, Any, List

from accelerate.utils import broadcast_object_list, gather, gather_object
from datasets import Dataset, IterableDataset
import torch
from torch import nn
from transformers import (
    PreTrainedModel,
    PreTrainedTokenizerBase,
    TrainerCallback,
    is_wandb_available,
    Trainer,
)
from transformers.utils import is_peft_available


from pathlib import Path
import sys
import os
# Add the parent directory to the Python path to access the sister directory
# At the top of grpo_multi.py
import sys
print("Python path:", sys.path)
import trl
print("Using trl from:", trl.__file__)



# Force a fresh import
import trl
print(f"Imported trl module: {trl}")
print(f"trl module path: {getattr(trl, '__file__', 'No __file__ attribute')}")

from trl import GRPOTrainer, GRPOConfig
from trl.data_utils import apply_chat_template, maybe_apply_chat_template
from trl.import_utils import is_rich_available
from trl.trainer.utils import pad
from trl.extras.profiling import profiling_context, profiling_decorator
from trl.trainer.utils import (
    generate_model_card,
    get_comet_experiment_url,
    pad,
    print_prompt_completions_sample,
    selective_log_softmax,
)


from .environment import Environment
from .GRPOEnvLogger import print_prompt_completions_sample
import numpy as np


if is_peft_available():
    from peft import PeftConfig # type: ignore

if is_wandb_available():
    import wandb

RewardFunc = Union[str, PreTrainedModel, Callable[[list, list], list[float]]]


def sample_geometric_bounded(p, max_value):
    while True:
        sample = np.random.geometric(p) - 1
        if sample <= max_value:
            return sample



class GRPOMultiTrainer(GRPOTrainer):
    def __init__(
            self,
            model: Union[str, PreTrainedModel],
            reward_funcs: Union[RewardFunc, list[RewardFunc]],
            args: Optional[GRPOConfig] = None,
            train_dataset: Optional[Union[Dataset, IterableDataset]] = None,
            eval_dataset: Optional[Union[Dataset, IterableDataset]] = None,
            processing_class: Optional[PreTrainedTokenizerBase] = None,
            callbacks: Optional[list[TrainerCallback]] = None,
            optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),
            peft_config: Optional["PeftConfig"] = None,
            turn_level_sampling: Optional[bool] = False,

            **kwargs,
    ):
        if not args.use_vllm: # type: ignore
            raise ValueError("vLLM must be enabled for GRPOMultiTrainer")
        if not (callable(reward_funcs) or (isinstance(reward_funcs, list) and all(callable(f) for f in reward_funcs))): 
            raise ValueError("reward_funcs must be a function or a list of functions. Use vLLM to host neural reward models.")

        self.turn_level_sampling = turn_level_sampling
        self._total_train_tokens = 0

        super().__init__(
            model=model,
            reward_funcs=reward_funcs,
            args=args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            processing_class=processing_class,
            callbacks=callbacks,
            optimizers=optimizers,
            peft_config=peft_config,
            **kwargs,
        )


    
    @profiling_decorator
    def _get_per_token_logps(self, model, input_ids, attention_mask, assistant_mask):
        """
        Compute the log probabilities for tokens generated by the assistant.
        
        Args:
            model: The model to compute log probabilities for
            input_ids: Token IDs of shape [B, L]
            attention_mask: Attention mask of shape [B, L]
            assistant_mask: Binary mask indicating which tokens were generated by the assistant [B, L]
            
        Returns:
            Log probabilities for the assistant-generated tokens
        """
        rank = self.accelerator.process_index
    
        # Shift assistant_mask right to align with the next token prediction
        shifted_assistant_mask = assistant_mask[:, 1:]  # Shape: [B, L-1]
        
        # Early check - if there are no assistant tokens, return zeros and avoid computation
        if shifted_assistant_mask.sum().item() == 0:
            return torch.zeros(
                (input_ids.size(0), shifted_assistant_mask.size(1)), 
                device=input_ids.device
            )
        
        # Get logits from the model for the entire sequence
        logits = model(input_ids=input_ids, attention_mask=attention_mask).logits
        
        # Shift logits left and input_ids right to align
        # This matches each token's logit with the next token's ID (standard LM setup)
        logits = logits[:, :-1, :]  # Shape: [B, L-1, V]
        shifted_input_ids = input_ids[:, 1:]  # Shape: [B, L-1]
        shifted_assistant_mask = assistant_mask[:, 1:]  # Shape: [B, L-1]
        
        # Divide logits by temperature
        logits = logits / self.temperature
        

        print(f"[Rank {rank}] Assistant mask sum: {shifted_assistant_mask.sum().item()} out of {shifted_assistant_mask.numel()}")
        
        # Compute log probabilities for each token
        per_token_logps = selective_log_softmax(logits, shifted_input_ids)
        
        # Apply the assistant mask
        masked_log_probs = per_token_logps * shifted_assistant_mask

        return masked_log_probs

    def _generate_and_score_completions(
         self, inputs: dict[str, Union[torch.Tensor, Any]]   
    ) -> dict[str, Union[torch.Tensor, Any]]:
        device = self.accelerator.device
        prompts = [x["prompt"] for x in inputs] # type: ignore

        ##if there is starting_agent in the inputs 
        starting_agent = [x["starting_agent"] for x in inputs if "starting_agent" in x]
        negotiation_roles = [x["negotiation_role"] for x in inputs if "negotiation_role" in x]
        game_configs = [x["game_config"] for x in inputs if "game_config" in x]
        starting_agent = gather_object(starting_agent)
        negotiation_roles = gather_object(negotiation_roles)
        game_configs = gather_object(game_configs)

        print(f"Starting agent: {starting_agent}")
        if len(starting_agent) > 0:
            starting_agent = starting_agent[0]
        else:
            starting_agent = None

        # Gather and broadcast the additional parameters

        if self.state.global_step != self._last_loaded_step:
            self._move_model_to_vllm()
            self._last_loaded_step = self.state.global_step

        # Gather the original prompts in message dict form, not the text form
        all_prompts_text = gather_object(prompts)

        prompts_2 = [x["prompt_2"] for x in inputs] # type: ignore
        all_prompts_text_2 = gather_object(prompts_2)

        rank = self.accelerator.process_index

    
        # Add rank to all print statements
        print(f"[Rank {rank}] Entering generating and scoring completions")

        if self.accelerator.is_main_process:

            # Initialize sampled_h with a default value of None
            sampled_h = None

            if self.turn_level_sampling:
                mode = "eval" if self.control.should_evaluate else "train"
                #Sample a random turn number 
                print(f"[Rank {rank}] Sampling turn number for {mode} set")
                if mode == "train":
                    #For now the number of conversation turns is fixed to 5 -> h ~ {0, 1, 2, 3, 4}
                    sampled_h = sample_geometric_bounded(p=0.3, max_value=4)
                    print(f"[Rank {rank}] Sampled turn number: {sampled_h}")

            # ordered_set_of_prompts = all_prompts_text[:: self.num_generations]

            with profiling_context(self, "vLLM.generate"):
                for attempt in range(5):
                    try:
                        client_response = self.vllm_client.generate(
                            prompts=all_prompts_text,
                            prompts_2=all_prompts_text_2,
                            n=self.num_generations,
                            repetition_penalty=self.repetition_penalty,
                            temperature=self.temperature,
                            top_p=self.top_p,
                            top_k=-1 if self.top_k is None else self.top_k,
                            min_p=0.0 if self.min_p is None else self.min_p,
                            max_tokens=self.max_completion_length,
                            guided_decoding_regex=self.guided_decoding_regex,
                            starting_agent=starting_agent,
                            sampled_h=sampled_h
                        )
                        break
                    except Exception as e:
                        print(f"Connection failed (attempt {attempt+1}/{5}): {e}")
                        if attempt < 4:
                            if attempt == 3:
                                print("Restarting vLLM client")
                                self.vllm_client = VLLMClient(
                                    self.args.vllm_server_host, self.args.vllm_server_port, connection_timeout=self.args.vllm_server_timeout
                                )
                            print(f"Waiting 120 seconds before retrying...")
                            time.sleep(120)
                        else:
                            print("Max retries reached. Giving up.")
                            raise
                   

            full_conversations = client_response["conversations"]
            token_ids_list = client_response["token_ids"]
            attention_mask_list = client_response["attention_masks"]
            assistant_mask_list = client_response["assistant_masks"]
            total_token_count = client_response["total_token_count"][0]
        else:
            full_conversations = [None] * len(all_prompts_text)
            token_ids_list = [None] * len(all_prompts_text)
            attention_mask_list = [None] * len(all_prompts_text)
            assistant_mask_list = [None] * len(all_prompts_text)
            total_token_count = 0


        full_conversations = broadcast_object_list(full_conversations, from_process=0)
        token_ids_list = broadcast_object_list(token_ids_list, from_process=0)
        attention_mask_list = broadcast_object_list(attention_mask_list, from_process=0)
        assistant_mask_list = broadcast_object_list(assistant_mask_list, from_process=0)
        #Total token count is a number that needs to be summed up across all processes

        process_slice = slice(
            self.accelerator.process_index * len(prompts),
            (self.accelerator.process_index + 1) * len(prompts),
        )

        full_conversations = full_conversations[process_slice]

        # Convert lists to tensors before stacking them
        token_ids_tensors = [torch.tensor(ids, device=device) for ids in token_ids_list[process_slice]]
        attention_mask_tensors = [torch.tensor(mask, device=device) for mask in attention_mask_list[process_slice]]
        assistant_mask_tensors = [torch.tensor(mask, device=device) for mask in assistant_mask_list[process_slice]]

        token_ids = torch.stack(token_ids_tensors, dim=0)
        attention_mask = torch.stack(attention_mask_tensors, dim=0)
        assistant_mask = torch.stack(assistant_mask_tensors, dim=0)
        #PRINT STARTING TOKENIZATION IN RANK x
        

        with torch.no_grad():
            # When using num_iterations == 1, old_per_token_logps == per_token_logps, so we can skip it's
            # computation here, and use per_token_logps.detach() instead.
            if self.num_iterations > 1:
                old_per_token_logps = self._get_per_token_logps(
                    self.model, token_ids, attention_mask, assistant_mask
                )
            else:
                old_per_token_logps = None

            if self.beta == 0.0:
                ref_per_token_logps = None
            elif self.ref_model is not None:
                ref_per_token_logps = self._get_per_token_logps(
                    self.ref_model, token_ids, attention_mask, assistant_mask
                )
            else:
                with self.accelerator.unwrap_model(self.model).disable_adapter():
                    ref_per_token_logps = self._get_per_token_logps(
                        self.model, token_ids, attention_mask, assistant_mask
                    )

        print("Calculating rewards")
        # use message dicts for reward function inputs
        rewards_per_func = torch.zeros(len(prompts), len(self.reward_funcs), device=device)
        for i, reward_func in enumerate(self.reward_funcs):
            # Repeat all input columns (but "prompt" and "completion") to match the number of generations
            keys = [key for key in inputs[0] if key not in ["prompt", "completion"]] # type: ignore
            reward_kwargs = {key: [example[key] for example in inputs] for key in keys} # type: ignore
            print("Negotiation roles:", negotiation_roles)
            print("Game configs:", game_configs)
            reward_result = reward_func(prompts=prompts, completions=full_conversations, get_full_info=True, negotiation_roles=negotiation_roles, game_configs=game_configs, **reward_kwargs)
            if isinstance(reward_result, tuple) and len(reward_result) == 2:
                output_reward_func, evaluations = reward_result
            else:
                output_reward_func = reward_result
                evaluations = None
            print("Output reward evaluations:", evaluations)
            rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)

        rewards_per_func = gather(rewards_per_func)

        print("Rewards per func:", rewards_per_func)

        # Apply weights to each reward function's output and sum
        rewards = (rewards_per_func * self.reward_weights.to(device).unsqueeze(0)).sum(dim=1)

        # Compute grouped-wise rewards
        mean_grouped_rewards = rewards.view(-1, self.num_generations).mean(dim=1) # type: ignore
        std_grouped_rewards = rewards.view(-1, self.num_generations).std(dim=1) # type: ignore

        # Normalize the rewards to compute the advantages
        mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(self.num_generations, dim=0) # type: ignore
        std_grouped_rewards = std_grouped_rewards.repeat_interleave(self.num_generations, dim=0) # type: ignore
        advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + 1e-4)


        # Slice to keep only the local part of the data
        process_slice = slice(
            self.accelerator.process_index * len(prompts),
            (self.accelerator.process_index + 1) * len(prompts),
        )
        advantages = advantages[process_slice]

        # Log the metrics
        mode = "eval" if self.control.should_evaluate else "train"

        # if mode == "train":
        #     self._total_train_tokens += self.accelerator.gather_for_metrics(attention_mask.sum()).sum().item()
        # self._metrics[mode]["num_tokens"] = [self._total_train_tokens]

        completion_length = self.accelerator.gather_for_metrics(assistant_mask.sum(1)).float().mean().item() # type: ignore
        self._metrics[mode]["completion_length"].append(completion_length)

        # Add tracking for total tokens and assistant tokens
        total_token_count = torch.tensor(total_token_count, device=device)
        if mode == "train":
            print(f"Total train tokens: {self._total_train_tokens}")
            self._total_train_tokens += self.accelerator.gather_for_metrics(total_token_count).sum().item()
            self._metrics[mode]["train_tokens"].append(self._total_train_tokens)

        reward_per_func = rewards_per_func.mean(0) # type: ignore
        for i, reward_func in enumerate(self.reward_funcs):
            if reward_func.__name__:
                reward_func_name = reward_func.__name__ # type: ignore
            else:
                reward_func_name = f"reward_func_{i}"
            self._metrics[mode][f"rewards/{reward_func_name}"].append(reward_per_func[i].item())

        self._metrics[mode]["reward"].append(rewards.mean().item())
        self._metrics[mode]["reward_std"].append(std_grouped_rewards.mean().item())

        if self.log_completions and self.state.global_step % self.args.logging_steps == 0:
            prompts_to_log = gather_object(prompts)
            completions_to_log = gather_object(full_conversations)
            rewards_to_log = rewards.tolist()

            if self.accelerator.is_main_process:
                if is_rich_available():
                    print_prompt_completions_sample(
                        prompts_to_log,
                        completions_to_log,
                        rewards_to_log,
                        self.state.global_step,
                    )
                if self.args.report_to and "wandb" in self.args.report_to and wandb.run is not None: # type: ignore
                    import pandas as pd

                    # For logging
                    table = {
                        "step": [str(self.state.global_step)] * len(rewards),
                        "prompt": prompts_to_log,
                        "completion": completions_to_log,
                        "reward": rewards.tolist(),
                    }
                    df = pd.DataFrame(table)
                    wandb.log({"completions": wandb.Table(dataframe=df)}) # type: ignore


        #free memory
        del full_conversations, rewards_per_func, rewards, mean_grouped_rewards, std_grouped_rewards
        torch.cuda.empty_cache()

        return {
            "token_ids": token_ids,
            "attention_mask": attention_mask,
            "assistant_mask": assistant_mask,
            "old_per_token_logps": old_per_token_logps,
            "ref_per_token_logps": ref_per_token_logps,
            "advantages": advantages,
        }
    
    @profiling_decorator
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        if return_outputs:
            raise ValueError("The GRPOTrainer does not support returning outputs")
        # Compute the per-token log probabilities for the model

        input_ids = inputs["token_ids"]
        attention_mask = inputs["attention_mask"]
        assistant_mask = inputs["assistant_mask"]


        per_token_logps = self._get_per_token_logps(model, input_ids, attention_mask, assistant_mask)

        # Compute the KL divergence between the model and the reference model
        if self.beta != 0.0:
            ref_per_token_logps = inputs["ref_per_token_logps"]
            per_token_kl = (
                torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - 1
            )

        # Compute the loss
        advantages = inputs["advantages"]
        # When using num_iterations == 1, old_per_token_logps == per_token_logps, so we can skip it's computation (see
        # _generate_and_score_completions) and use per_token_logps.detach() instead.
        old_per_token_logps = inputs["old_per_token_logps"] if self.num_iterations > 1 else per_token_logps.detach()
        coef_1 = torch.exp(per_token_logps - old_per_token_logps)
        coef_2 = torch.clamp(coef_1, 1 - self.epsilon_low, 1 + self.epsilon_high)
        per_token_loss1 = coef_1 * advantages.unsqueeze(1)
        per_token_loss2 = coef_2 * advantages.unsqueeze(1)
        per_token_loss = -torch.min(per_token_loss1, per_token_loss2)
        if self.beta != 0.0:
            per_token_loss = per_token_loss + self.beta * per_token_kl


        shifted_assistant_mask = assistant_mask[:, 1:]  # Shape: [B, L-1]

        loss = (per_token_loss * shifted_assistant_mask).sum() / shifted_assistant_mask.sum()

        # Log the metrics
        mode = "eval" if self.control.should_evaluate else "train"

        if self.beta != 0.0:
            mean_kl = (per_token_kl * shifted_assistant_mask).sum() / shifted_assistant_mask.sum()
            self._metrics[mode]["kl"].append(self.accelerator.gather_for_metrics(mean_kl).mean().item())

        is_clipped = (coef_1 < (1 - self.epsilon_low)) | (coef_1 > (1 + self.epsilon_high))
        clip_ratio = (is_clipped * shifted_assistant_mask).sum() / shifted_assistant_mask.sum()
        self._metrics[mode]["clip_ratio"].append(self.accelerator.gather_for_metrics(clip_ratio).mean().item())


        print("Calculated loss:", loss)
        return loss
    
